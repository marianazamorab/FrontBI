{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c7bfb0-5843-481f-94a5-379df29108f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()  # for plot styling\n",
    "import contractions\n",
    "import inflect\n",
    "import re, string, unicodedata\n",
    "from joblib import dump, load\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2479fb9-fe0e-4194-a795-63ae96c671b5",
   "metadata": {},
   "source": [
    "# Transformadores Personalizados\n",
    "Inicialmente se crearon 2 transformadores con las operaciones personalizadas usadas en la solución propuesta. El primer transformer, llamado LimpiezaTransformer, se encarga de preparar los datos en cuanto a formato, validez de los caracteres, mayúsculas y minúsculas, puntuación y demás."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34befb5b-5b26-4e4c-9ee1-d8f5fd5981f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimpiezaTransformer(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def limpiar_study(self,value):\n",
    "        return value.replace('study interventions are ', '')\n",
    "    \n",
    "    def remove_non_ascii(self,words):\n",
    "        \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            new_words.append(new_word)\n",
    "        return new_words\n",
    "\n",
    "    def to_lowercase(self,words):\n",
    "        \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "        new_words =[]\n",
    "        for word in words:\n",
    "            new_word = word.lower()\n",
    "            new_words.append(new_word)\n",
    "        return new_words\n",
    "\n",
    "    def remove_punctuation(self,words):\n",
    "        \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "            if new_word != '':\n",
    "                new_words.append(new_word)\n",
    "        return new_words\n",
    "\n",
    "    def replace_numbers(self,words):\n",
    "        \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "        p = inflect.engine()\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if word.isdigit():\n",
    "                new_word = p.number_to_words(word)\n",
    "                new_words.append(new_word)\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        return new_words\n",
    "\n",
    "    def remove_stopwords(self,words):\n",
    "        \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        for word in words:\n",
    "            if word not in stopwords:\n",
    "                new_words.append(word)\n",
    "        return new_words\n",
    "\n",
    "    def preprocessing(self,words):\n",
    "        words = self.to_lowercase(words)\n",
    "        words = self.replace_numbers(words)\n",
    "        words = self.remove_punctuation(words)\n",
    "        words = self.remove_non_ascii(words)\n",
    "        words = self.remove_stopwords(words)\n",
    "        return words\n",
    "    def transform(self, X, y=None):\n",
    "        X_ = X.copy()\n",
    "        X_add = X_[\"study_and_condition\"].str.split('.', 1, expand=True)\n",
    "        X_add.columns = ['study', 'condition']\n",
    "        X_ = pd.concat([X_, X_add], axis=1)\n",
    "        X_.drop('study_and_condition', axis=1, inplace=True)\n",
    "        X_['study'] = X_['study'].map(self.limpiar_study)\n",
    "        X_['condition'] = X_['condition'].apply(contractions.fix) #Aplica la corrección de las contracciones\n",
    "        X_['words'] = X_['condition'].apply(word_tokenize).apply(self.preprocessing) #Aplica la eliminación del ruido\n",
    "        return X_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411886d5-0e73-46e6-b047-8456a41b1d90",
   "metadata": {},
   "source": [
    "El segundo transformer, llamado NormalizacionTransformer, se encarga de realizar el stemming y lematización de las palabras en la entrada de tal manera que las palabras puedan ser relacionables en el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ec4bed-b70a-4ceb-a37b-8e5bfa325b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizacionTransformer(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def stem_words(self,words):\n",
    "        \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "        stemmer = LancasterStemmer()\n",
    "        stems = []\n",
    "        for word in words:\n",
    "            stem = stemmer.stem(word)\n",
    "            stems.append(stem)\n",
    "        return stems\n",
    "\n",
    "    def lemmatize_verbs(self,words):\n",
    "        \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmas = []\n",
    "        for word in words:\n",
    "            lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "            lemmas.append(lemma)\n",
    "        return lemmas\n",
    "\n",
    "    def stem_and_lemmatize(self,words):\n",
    "        stems = self.stem_words(words)\n",
    "        lemmas = self.lemmatize_verbs(words)\n",
    "        return stems + lemmas\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_ = X.copy()\n",
    "        X_['words'] = X_['words'].apply(self.stem_and_lemmatize) #Aplica lematización y Eliminación de Prefijos y Sufijos.\n",
    "        X_['words'] = X_['words'].apply(lambda x: ' '.join(map(str, x)))\n",
    "        return X_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd3c2e6-b154-4994-aaad-4aa8a49d50d3",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "Una vez definidos los transformadores, se creó un pipeline que ejecuta primero la limpieza y luego la normalización. Después, este pipeline se exportó mediante joblib como un pickle para propósitos de la conexión con el API desarrollado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c057a5-a674-41b4-8232-3be67e7f6e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('limpieza', LimpiezaTransformer()),\n",
    "                       ('normalizar', NormalizacionTransformer())])\n",
    "\n",
    "joblib.dump(pipe, 'pipeline1.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46777c3-5d3f-4240-9522-e847abc3dd43",
   "metadata": {},
   "source": [
    "# Uso en ambiente de producción\n",
    "Una vez exportado el pipeline, también se tuvo que exportar el vectorizador y el modelo usados en la entrega previa. En este caso, el vectorizador tiene un rol muy importante dado que permite que un conjunto de palabras sean transformadas en un vector de la misma forma que los usados en el entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ca040e-f5c2-4367-9046-9e56611f8c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1=load('pipeline1.joblib')\n",
    "vectorizer=load('vectorizer.joblib')\n",
    "model=load('svcmodel.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df86b42-fecd-4b65-a627-0758d6cb7bdb",
   "metadata": {},
   "source": [
    "Para poder predecir la elegibilidad de un nuevo registro se puede usar una función como la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0120683-cb51-4952-8b93-15226c8815fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eligibility(register):\n",
    "    registrotrans = pipe1.transform(register)\n",
    "    registrotrans = vectorizer.transform(registrotrans['words'])\n",
    "    prediction = model.predict(registrotrans)[0]\n",
    "    proba = model.predict_proba(registrotrans)[0]\n",
    "    if prediction:\n",
    "        proba = round(proba[1],3)\n",
    "    else:\n",
    "        proba = round(proba[0],3)\n",
    "    return ['Eligible' if prediction == 1 else 'Not eligible',proba]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a1ddd5-ddf0-4e67-a1af-972d14153452",
   "metadata": {},
   "source": [
    "Si se tiene el texto de un registro en una variable, en este caso study, se puede probar de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce68d7e-908b-447a-8322-13d7f328c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probar la elegibilidad de un registro nuevo\n",
    "eligibility(pd.DataFrame({'study_and_condition':[study]}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
